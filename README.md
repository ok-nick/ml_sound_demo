# `ml_sound_demo`
Machine learning demonstration using Whisper for audio transcription and Wav2Vec2 for audio classification.

## Outstanding Questions
* How are projects similar to this usually structured? File structure? API structure?
* Which libraries are *actually* needed?
* How can I make the code better/more optimal? Which functions to use?
    * What are the standard conventions?
